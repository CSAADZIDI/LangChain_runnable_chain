{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f4459b",
   "metadata": {},
   "source": [
    "![LangChain](img/langchain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511a0f0",
   "metadata": {},
   "source": [
    "**LangChain** permet d‚Äôencha√Æner facilement diff√©rents composants de traitement dans un **pipeline unifi√©**. Ces composants ‚Äî qu‚Äôil s‚Äôagisse d‚Äôun **prompt**, d‚Äôun **mod√®le de langage** ou d‚Äôun **outil externe** ‚Äî sont tous trait√©s comme des `Runnable`, c‚Äôest-√†-dire des **blocs interop√©rables pouvant √™tre connect√©s les uns aux autres**.\n",
    "\n",
    "Gr√¢ce √† cette architecture, il devient simple de construire des cha√Ænes logiques de traitement par exemple :  \n",
    "\n",
    "> **g√©n√©rer un prompt** ‚Üí **l‚Äôenvoyer √† un LLM** ‚Üí **interpr√©ter la r√©ponse** ‚Üí **puis appeler une API ou une fonction locale**\n",
    "\n",
    "C'est avec le ***LangChain Expression Language*** (LCEL) que nous pouvons cha√Æner les composants via l‚Äôop√©rateur `|` (le pipe) et d‚Äôex√©cuter le tout de mani√®re uniforme avec `.invoke()`.\n",
    "\n",
    "Gr√¢ce aux `chains`, nous pouvons r√©sumer **Langchain** √† ceci :  \n",
    "\n",
    "> Bo√Æte √† outils pour cr√©er des pipelines modulaires, r√©utilisables et tra√ßables autour des mod√®les de langage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaee202",
   "metadata": {},
   "source": [
    "![Chains](img/chains.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7eb01a",
   "metadata": {},
   "source": [
    "# 1. Chargement du mod√®le LLM local\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafb689",
   "metadata": {},
   "source": [
    "Dans cette section, nous chargeons un mod√®le de langage local gr√¢ce √† **Ollama**. Cela permet de travailler avec un **LLM directement sur notre machine**, sans connexion √† une API externe.\n",
    "\n",
    "Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d‚Äôinteragir facilement avec un mod√®le comme llama3 d√©j√† t√©l√©charg√© via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7301c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnableBranch\n",
    "\n",
    "\n",
    "# Chargement des cl√©s d'API se trouvant dans le fichier .env.  \n",
    "# Ceci permet d'utiliser des mod√®les en ligne comme gpt-x, deepseek-x, etc...\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model = ChatOllama(model=\"llama3\")\n",
    "#model = ChatDeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a0cd4",
   "metadata": {},
   "source": [
    "# 2. Cha√Æne basique\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc2daf",
   "metadata": {},
   "source": [
    "Une cha√Æne de traitement simple peut √™tre construite en combinant un prompt structur√© avec un mod√®le de langage √† l‚Äôaide du syst√®me de cha√Ænage de LangChain.  \n",
    "Ce type de cha√Æne permet de cr√©er un dialogue en d√©finissant plusieurs r√¥les (comme system et human) et en injectant dynamiquement des valeurs dans le prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da43aef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Un exercice classique !\n",
       "\n",
       "Le double de 4 est √©gal √† : 4 √ó 2 = 8\n",
       "\n",
       "Et le double de 2 est √©gal √† : 2 √ó 2 = 4\n",
       "\n",
       "Voil√† les r√©ponses !"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On d√©finit une liste de messages structur√©s pour guider le comportement du mod√®le.\n",
    "# ‚ö†Ô∏è Ici, on utilise des TUPLES (r√¥le, message avec variables), c‚Äôest n√©cessaire pour que l‚Äôinterpolation des variables fonctionne avec from_messages().\n",
    "# ‚ö†Ô∏è L'interpolation avec des objets comme `HumanMessage(content=\"...\")` ou `SystemMessage(content=\"...\")` ne fonctionne PAS directement avec from_messages().\n",
    "# Ces objets sont con√ßus pour des messages d√©j√† complets, pas des templates avec des variables.\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\"),\n",
    "    (\"human\", \"Calcule le double de {value_1}, puis celui de {value_2}\")\n",
    "])\n",
    "\n",
    "# √âquivalent d'un template √† r√¥le unique\n",
    "# template = \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine. Calcule le double de {value_1}, puis celui de {value_2}.\"\n",
    "# prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# On relie le prompt au mod√®le √† l‚Äôaide de l‚Äôop√©rateur |\n",
    "chain = prompt_template | model\n",
    "\n",
    "# On fournit des valeurs aux variables d√©finies dans le prompt\n",
    "result = chain.invoke({\"value_1\": 4, \"value_2\": 2})\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e062a",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a840f",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Cr√©ez un prompt qui demande √† un mod√®le de d√©finir un mot donn√©, dans un style p√©dagogique.\n",
    "\n",
    "1.\tUtilisez ChatPromptTemplate.from_messages() pour d√©finir un prompt structur√© avec :\n",
    "- un message system : l‚ÄôIA est un professeur d'un domaine particulier qui explique simplement.\n",
    "- un message human : l‚Äôutilisateur demande la d√©finition d‚Äôun mot particulier.\n",
    "2.\tRelie ce prompt √† un mod√®le avec l‚Äôop√©rateur |.\n",
    "3.\tUtilise .invoke() pour tester le prompt avec plusieurs disciplines et th√®mes diff√©rents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d35f236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Quel belles qualit√© que la gentillesse !\n",
       "\n",
       "La gentillesse (ou tendresse) est une qualit√© morale qui consiste √† montrer de l'amiti√©, de la compassion et de la d√©licatesse envers autrui. C'est un trait de caract√®re qui fait preuve de soins, d'attention et de sollicitude envers les autres.\n",
       "\n",
       "La gentillesse peut s'exprimer de diff√©rentes mani√®res, comme :\n",
       "\n",
       "* La parole douce et r√©confortante\n",
       "* Les gestes d√©licats et pr√©venants\n",
       "* L'√©coute attentive et compr√©hensive\n",
       "* Le sourire chaleureux et encourageant\n",
       "\n",
       "La gentillesse est un √©l√©ment essentiel pour b√¢tir des relations saines et durables avec autrui. Elle permet de cr√©er un climat de confiance, d'acceptation et de bienveillance.\n",
       "\n",
       "En tant que p√©dagogue, je peux dire que la gentillesse est un enseignement essentiel que j'ai appris √† partager avec mes √©l√®ves, car elle est une qualit√© qui rend les relations plus harmonieuses et enrichissantes pour tous."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un p√©dagogue.\"),\n",
    "    (\"human\", \"D√©finis ce mot {mot}\")\n",
    "])\n",
    "\n",
    "# √âquivalent d'un template √† r√¥le unique\n",
    "# template = \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine. Calcule le double de {value_1}, puis celui de {value_2}.\"\n",
    "# prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# On relie le prompt au mod√®le √† l‚Äôaide de l‚Äôop√©rateur |\n",
    "chain = prompt_template | model\n",
    "\n",
    "# On fournit des valeurs aux variables d√©finies dans le prompt\n",
    "result = chain.invoke({\"mot\": \"gentillesse\"})\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3e0b3",
   "metadata": {},
   "source": [
    "# 3. Cha√Æne √©tendue (s√©quence de runnables)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab574c",
   "metadata": {},
   "source": [
    "L‚Äôun des atouts majeurs de LangChain r√©side dans son syst√®me de **cha√Ænes composables**, o√π chaque composant du pipeline est un `Runnable`. Gr√¢ce √† l‚Äôop√©rateur `|`, on peut encha√Æner autant d'√©tapes de traitement que voulu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049d905",
   "metadata": {},
   "source": [
    "### 3.1 Runnable built-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c81a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Un exercice classique !\n",
       "\n",
       "Le double de 4 est √©gal √† :\n",
       "\n",
       "4 x 2 = 8\n",
       "\n",
       "Et maintenant, le double de 2 est √©gal √† :\n",
       "\n",
       "2 x 2 = 4\n",
       "\n",
       "Voil√† !"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\"),\n",
    "    (\"human\", \"Calcule le double de {value_1}, puis celui de {value_2}\")\n",
    "])\n",
    "\n",
    "# Ce parseur prend la sortie brute du mod√®le (souvent du texte) et la convertit en cha√Æne de caract√®res simple pour faciliter la suite.\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Encha√Ænement de runnables\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "result = chain.invoke({\"value_1\": 4, \"value_2\": 2})\n",
    "\n",
    "# Affichage du r√©sultat retourn√© par le mod√®le apr√®s parsing. Plus besoin du `.content`\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f877d27",
   "metadata": {},
   "source": [
    "### 3.2 Runnable custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03672db2",
   "metadata": {},
   "source": [
    "Langchain offre non seulement d‚Äôutiliser des composants pr√©d√©finis (LLMs, parsers, prompts‚Ä¶) comme √©voqu√© pr√©c√©demment, mais aussi de d√©finir facilement ses propres blocs de traitement.\n",
    "\n",
    "Gr√¢ce √† la classe `RunnableLambda`, on peut transformer n‚Äôimporte quelle fonction Python en un maillon de la cha√Æne. Cela ouvre la porte √† un nombre infini de transformations : nettoyage de texte, post-traitement, extraction de donn√©es, formatage, journalisation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e85ab31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "EXCELLENT EXERCICE POUR COMMENCER !\n",
       "\n",
       "LE DOUBLE DE 4 EST √âGAL √Ä : 4 √ó 2 = 8\n",
       "\n",
       "ET LE DOUBLE DE 2 EST √âGAL √Ä : 2 √ó 2 = 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\"),\n",
    "    (\"human\", \"Calcule le double de {value_1}, puis celui de {value_2}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "uppercase = RunnableLambda(lambda x: x.upper()) # Runnable custom pour transformer la sortie en majuscules\n",
    "\n",
    "# Encha√Ænement de runnables\n",
    "chain = prompt_template | model | parser | uppercase\n",
    "\n",
    "result = chain.invoke({\"value_1\": 4, \"value_2\": 2})\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333bf7b",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57787ca8",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Cr√©ez un pipeline qui r√©pond √† des questions clients ou formule des messages marketing. Il faut que ces r√©ponses soient :\n",
    "- stylis√©es,\n",
    "- enrichies,\n",
    "- adapt√©es √† diff√©rents formats de publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5d93c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**WHY CHOOSE OUR PRODUCT?**\n",
       "\n",
       "IN TODAY'S FAST-PACED WORLD, YOU DESERVE A SOLUTION THAT NOT ONLY MEETS BUT EXCEEDS YOUR EXPECTATIONS. THAT'S WHY WE'RE PROUD TO OFFER A PRODUCT THAT STANDS OUT FROM THE REST.\n",
       "\n",
       "**INNOVATIVE TECHNOLOGY**: OUR PRODUCT IS DESIGNED WITH CUTTING-EDGE TECHNOLOGY THAT MAKES IT EASY TO USE AND MAINTAIN. WITH ITS INTUITIVE INTERFACE AND USER-FRIENDLY FEATURES, YOU'LL BE UP AND RUNNING IN NO TIME.\n",
       "\n",
       "**COST-EFFECTIVE**: DITCH THOSE PESKY SUBSCRIPTION FEES! OUR PRODUCT IS A ONE-TIME INVESTMENT THAT WILL SAVE YOU MONEY IN THE LONG RUN. PLUS, OUR AFFORDABLE PRICING MEANS YOU CAN ENJOY ALL THE BENEFITS WITHOUT BREAKING THE BANK.\n",
       "\n",
       "**UNPARALLELED SUPPORT**: AT [YOUR COMPANY], WE'RE COMMITTED TO YOUR SUCCESS. OUR DEDICATED SUPPORT TEAM IS ALWAYS AVAILABLE TO ANSWER QUESTIONS, PROVIDE GUIDANCE, AND HELP YOU GET THE MOST OUT OF YOUR PRODUCT.\n",
       "\n",
       "**CUSTOMIZABLE SOLUTIONS**: WE UNDERSTAND THAT EVERY CUSTOMER HAS UNIQUE NEEDS. THAT'S WHY OUR PRODUCT OFFERS FLEXIBLE CONFIGURATIONS AND CUSTOMIZATION OPTIONS TO ENSURE IT FITS YOUR SPECIFIC REQUIREMENTS.\n",
       "\n",
       "**COMPLIANCE AND SECURITY**: REST ASSURED THAT OUR PRODUCT IS COMPLIANT WITH ALL RELEVANT REGULATIONS AND STANDARDS. PLUS, WE TAKE SECURITY SERIOUSLY, WITH ROBUST MEASURES IN PLACE TO PROTECT YOUR DATA AND INFORMATION.\n",
       "\n",
       "**RESULTS-DRIVEN**: OUR PRODUCT HAS BEEN PROVEN TO DELIVER RESULTS. WITH ITS POWERFUL FEATURES AND USER-FRIENDLY INTERFACE, YOU'LL BE ABLE TO ACHIEVE YOUR GOALS FASTER AND MORE EFFICIENTLY THAN EVER BEFORE.\n",
       "\n",
       "**JOIN THE RANKS OF SATISFIED CUSTOMERS**: DON'T JUST TAKE OUR WORD FOR IT! THOUSANDS OF SATISFIED CUSTOMERS HAVE ALREADY EXPERIENCED THE BENEFITS OF OUR PRODUCT. JOIN THEIR RANKS AND DISCOVER WHY WE'RE THE GO-TO CHOICE FOR [YOUR INDUSTRY/FIELD].\n",
       "\n",
       "SO, WHAT ARE YOU WAITING FOR? CHOOSE OUR PRODUCT TODAY AND START ACHIEVING YOUR GOALS WITH CONFIDENCE!\n",
       "\n",
       "**KEY TAKEAWAYS:**\n",
       "\n",
       "INNOVATIVE TECHNOLOGY\n",
       "COST-EFFECTIVE\n",
       "UNPARALLELED SUPPORT\n",
       "CUSTOMIZABLE SOLUTIONS\n",
       "COMPLIANCE AND SECURITY\n",
       "RESULTS-DRIVEN\n",
       "SATISFIED CUSTOMERS"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un en service client et en marketing.\"),\n",
    "    (\"human\", \"R√©ponds avec des r√©ponses stylis√©es et enrichis et adapt√©es √† diff√©rents formats de publication {question}\")\n",
    "])\n",
    "question1 = \"Pourquoi dois-je choisir votre produit\"\n",
    "\n",
    "parser = StrOutputParser()\n",
    "uppercase = RunnableLambda(lambda x: x.upper()) # Runnable custom pour transformer la sortie en majuscules\n",
    "\n",
    "# Encha√Ænement de runnables\n",
    "chain = prompt_template | model | parser | uppercase\n",
    "\n",
    "result = chain.invoke({\"question\":question1})\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21b621",
   "metadata": {},
   "source": [
    "# 4. Cha√Ænes parall√®les\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497574e0",
   "metadata": {},
   "source": [
    "### 4.1 Cha√Ænes parall√®les avec post-traitement externe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce763ed",
   "metadata": {},
   "source": [
    "Dans LangChain, il est possible d‚Äôex√©cuter plusieurs **cha√Ænes de traitement en parall√®le** √† l‚Äôaide du composant `RunnableParallel`. Cela permet, par exemple, d‚Äôeffectuer plusieurs op√©rations ind√©pendantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e798a6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©sultat de l'addition :\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Facile ! 10 + 4 = 14"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultat de la soustraction :\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Un classique !\n",
       "\n",
       "Si je soustrai 10 de 4, j'obtiens :\n",
       "\n",
       "4 - 10 = -6"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_role = (\"system\", \"Tu es un expert en math√©matiques.\")\n",
    "\n",
    "# Prompt pour additionner\n",
    "prompt_add = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"Additionne {value_1} √† {value_2}.\")\n",
    "])\n",
    "\n",
    "# Prompt pour soustraire\n",
    "prompt_substract = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"Soustrais {value_1} de {value_2}.\")\n",
    "])\n",
    "\n",
    "# Cha√Ænes s√©par√©es\n",
    "chain_add = prompt_add | model | StrOutputParser()\n",
    "chain_substract = prompt_substract | model | StrOutputParser()\n",
    "\n",
    "# Traitement parall√®le √† ex√©cuter\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"add\": chain_add,\n",
    "    \"substract\": chain_substract\n",
    "})\n",
    "\n",
    "# M√™me jeu de donn√©es utilis√© pour les deux cha√Ænes\n",
    "inputs = {\"value_1\": 10, \"value_2\": 4}\n",
    "\n",
    "# Ex√©cution des traitements en parall√®le\n",
    "result = parallel_chain.invoke(inputs)\n",
    "\n",
    "# Affichage\n",
    "print(\"R√©sultat de l'addition :\\n\")\n",
    "display(Markdown(result[\"add\"]))\n",
    "print(\"\\nR√©sultat de la soustraction :\\n\")\n",
    "display(Markdown(result[\"substract\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8abc614",
   "metadata": {},
   "source": [
    "### 4.2 Cha√Ænes parall√®les avec post-traitement int√©gr√© dans la cha√Æne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1247d0",
   "metadata": {},
   "source": [
    "Pour √©viter de manipuler manuellement les r√©sultats (comme result[\"add\"] ou result[\"substract\"]), il est possible d‚Äôajouter un bloc de post-traitement directement √† la fin de la cha√Æne parall√®le gr√¢ce √† RunnableLambda.\n",
    "\n",
    "Cette approche permet de :\n",
    "- structurer la sortie de mani√®re centralis√©e,\n",
    "- int√©grer la logique m√©tier ou d‚Äôaffichage directement dans le pipeline.\n",
    "\n",
    "C‚Äôest une bonne pratique lorsqu‚Äôon souhaite rendre une cha√Æne modulaire, maintenable et r√©utilisable dans un syst√®me plus large (ex. : API, application, chatbot‚Ä¶)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_role = (\"system\", \"Tu es un expert en math√©matiques.\")\n",
    "\n",
    "# Prompts\n",
    "prompt_add = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"Additionne {value_1} √† {value_2}.\")\n",
    "])\n",
    "\n",
    "prompt_substract = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"Soustrais {value_1} de {value_2}.\")\n",
    "])\n",
    "\n",
    "# Cha√Ænes\n",
    "chain_add = prompt_add | model | StrOutputParser()\n",
    "chain_substract = prompt_substract | model | StrOutputParser()\n",
    "\n",
    "# Traitement parall√®le\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"addition\": chain_add,\n",
    "    \"soustraction\": chain_substract\n",
    "})\n",
    "\n",
    "# Post-traitement avec RunnableLambda\n",
    "postprocess = RunnableLambda(lambda result: \n",
    "f\"\"\"R√©sultats du traitement parall√®le :\n",
    "- Addition : {result[\"addition\"].strip()}\n",
    "- Soustraction : {result[\"soustraction\"].strip()}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Cha√Æne finale\n",
    "full_chain = parallel_chain | postprocess\n",
    "\n",
    "# Entr√©e\n",
    "inputs = {\"value_1\": 10, \"value_2\": 4}\n",
    "\n",
    "# R√©sultat\n",
    "result = full_chain.invoke(inputs)\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ada11a",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e5ddf",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Construire une mini-analyseur de texte. √Ä partir d‚Äôun m√™me paragraphe, nous voulons :\n",
    "- R√©sumer le texte\n",
    "- Extraire les mots-cl√©s\n",
    "- D√©tecter la langue\n",
    "- Analyser le sentiment\n",
    "\n",
    "Vous pouvez suivre ce sch√©ma :\n",
    "1. Cr√©er les prompts\n",
    "2. Cr√©er les cha√Ænes\n",
    "3. Assembler les cha√Ænes\n",
    "4. Pr√©parer les inputs\n",
    "5. Lancer le traitement et afficher les r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd7db21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "R√©sultats du traitement parall√®le :\n",
       "- summary : A short and sweet one!\n",
       "\n",
       "The text appears to be a poetic expression of the desire for freedom and elevation. The speaker is expressing a deep longing to rise above and soar to great heights, both physically and metaphorically.\n",
       "\n",
       "The use of \"fly\" as a verb emphasizes the idea of movement and propulsion upwards, suggesting a sense of liberation and escape from the constraints of the mundane world. The repetition of \"so high, so high\" drives home the intensity of this desire for ascension, implying that the speaker is willing to push beyond any limits or boundaries in order to achieve this goal.\n",
       "\n",
       "Overall, the text conveys a sense of yearning for transcendence and the thrill of exploring new horizons.\n",
       "- keywords : Les mots cl√©s de ce texte sont :\n",
       "\n",
       "* Fly\n",
       "* High\n",
       "- language : Une phrase courte et expressive !\n",
       "\n",
       "Selon mes analyses, la langue du texte est l'anglais. En effet, les mots \"I\", \"want\", \"to\", \"fly\", \"high\" sont des termes couramment utilis√©s en anglais. La grammaire et le vocabulaire employ√©s dans cette phrase sont √©galement caract√©ristiques de la langue anglaise.\n",
       "- sentiment_analysis : Un texte court et puissant !\n",
       "\n",
       "En analysant le sentiment de ce texte, je dirai que c'est principalement positif et √©nergique.\n",
       "\n",
       "La phrase \"I want to fly\" exprime une aspiration forte et un d√©sir d'√©l√©vation, de libert√© et de pouvoir. Le mot \"fly\" a √©galement une connotation de lib√©ration et de d√©livrance, ce qui renforce l'impression que le sentiment est positif.\n",
       "\n",
       "L'ajout de \"fly high, so high\" renforce cette impression, car il sugg√®re une aspiration √† atteindre des hauteurs exceptionnelles, ce qui √©voque une sensation d'exaltation et de triomphe. Le mot \"high\" a √©galement un sens de grandeur et de puissance, ce qui renforce l'impression que le sentiment est fort et dominant.\n",
       "\n",
       "En r√©sum√©, le sentiment de ce texte est principalement positif, √©nergique et aspiratif, avec une touche d'exaltation et de lib√©ration.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_role = (\"system\", \"Tu es un expert en analyse de texte.\")\n",
    "\n",
    "# Prompts\n",
    "prompt_summary = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"R√©sume le texte {texte1}.\")\n",
    "])\n",
    "\n",
    "prompt_keywords = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"Extrais les mots cl√©s de ce texte {texte1}.\")\n",
    "])\n",
    "\n",
    "prompt_language = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"D√©tecte la langue du texte {texte1}.\")\n",
    "])\n",
    "\n",
    "prompt_sentiment_analysis = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"Analyse le sentiment via ce texte {texte1}.\")\n",
    "])\n",
    "\n",
    "# Cha√Ænes\n",
    "chain_summary = prompt_summary | model | StrOutputParser()\n",
    "chain_keywords = prompt_keywords | model | StrOutputParser()\n",
    "chain_language = prompt_language | model | StrOutputParser()\n",
    "chain_sentiments_analysis = prompt_sentiment_analysis | model | StrOutputParser()\n",
    "\n",
    "\n",
    "# Traitement parall√®le\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"summary\": chain_summary,\n",
    "    \"keywords\": chain_keywords,\n",
    "    \"language\" : chain_language,\n",
    "    \"sentiment_analysis\" : chain_sentiments_analysis\n",
    "})\n",
    "\n",
    "# Post-traitement avec RunnableLambda\n",
    "postprocess = RunnableLambda(lambda result: \n",
    "f\"\"\"R√©sultats du traitement parall√®le :\n",
    "- summary : {result[\"summary\"].strip()}\n",
    "- keywords : {result[\"keywords\"].strip()}\n",
    "- language : {result[\"language\"].strip()}\n",
    "- sentiment_analysis : {result[\"sentiment_analysis\"].strip()}\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Cha√Æne finale\n",
    "full_chain = parallel_chain | postprocess\n",
    "\n",
    "# Entr√©e\n",
    "inputs = {\"texte1\": \" I want to fly, fly high, so high\"}\n",
    "\n",
    "# R√©sultat\n",
    "result = full_chain.invoke(inputs)\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca6e28",
   "metadata": {},
   "source": [
    "# 5. Branches conditionnelles\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d670faf9",
   "metadata": {},
   "source": [
    "Il est possible de d√©finir des chemins conditionnels dans un pipeline, on parle alors de branche conditionnelle.\n",
    "\n",
    "Gr√¢ce √† `RunnableBranch`, il est possible de router dynamiquement la sortie d‚Äôun composant (comme un LLM) vers diff√©rents traitements en fonction de son contenu ou de n‚Äôimporte quelle r√®gle m√©tier.\n",
    "\n",
    "Dans l'exemple qui suit :\n",
    "\n",
    "1. On demande au LLM de calculer le double d‚Äôune valeur et de retourner uniquement un r√©sultat num√©rique brut.\n",
    "2. On analyse ce r√©sultat :\n",
    "- Si le r√©sultat est sup√©rieur ou √©gal √† 100, on le met en majuscules et on affiche un message adapt√©.\n",
    "- Sinon, on l‚Äôaffiche en minuscules avec un message diff√©rent.\n",
    "3. Tout cela est encapsul√© dans une cha√Æne principale.\n",
    "\n",
    "Ce m√©canisme est extr√™mement utile pour adapter dynamiquement le comportement d‚Äôune IA √† diff√©rents contextes : affichage, r√®gles m√©tier, logique m√©tier avanc√©e ou traitements sp√©cialis√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\"),\n",
    "    (\"human\", \"Calcule le double de {value}. Retourne uniquement le r√©sulat sous forme de nombre, sans explications ou autres types de texte.\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "base_chain = prompt_template | model | parser\n",
    "\n",
    "# Runnables de traitement et de formatage\n",
    "uppercase = RunnableLambda(lambda x: f\"Le r√©sultat est {x} (>= 100), transformation en majuscules.\".upper())\n",
    "lowercase = RunnableLambda(lambda x: f\"Le r√©sultat est {x} (< 100), tout en minuscules.\".lower())\n",
    "\n",
    "# Branche selon le contenu g√©n√©r√©\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: int(x) >= 100, uppercase),\n",
    "    lowercase\n",
    ")\n",
    "\n",
    "# Cha√Æne compl√®te : on applique d‚Äôabord le LLM, puis on branche\n",
    "chain = base_chain | branch\n",
    "\n",
    "result = chain.invoke({\"value\": 1})\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef1cf1d",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36febf86",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Sur une fiche produit e-commerce, les clients laissent des commentaires vari√©s. L‚Äôobjectif est de construire une cha√Æne intelligente capable de r√©pondre √† chacun de ces commentaires de mani√®re empathique et appropri√©e, sans intervention humaine.\n",
    "\n",
    "Construire une cha√Æne LangChain **enti√®rement automatis√©e**, dans laquelle un mod√®le de langage (LLM) :\n",
    "\n",
    "1.\tAnalyse un commentaire client brut,\n",
    "2.\tD√©tecte la tonalit√© du message (positive, negative, neutral),\n",
    "3.\tEt g√©n√®re une r√©ponse adapt√©e, en s√©lectionnant dynamiquement le bon ton de r√©ponse via un branchement conditionnel (RunnableBranch).\n",
    "\n",
    "**Exemple :**\n",
    "\n",
    "\"J‚Äôai bien re√ßu le produit, mais l‚Äôemballage √©tait ab√Æm√©.\"\n",
    "\n",
    "‚û°Ô∏è Le LLM doit d√©tecter un sentiment n√©gatif, puis router vers une r√©ponse du type :\n",
    "\n",
    "\"Nous sommes d√©sol√©s d‚Äôapprendre cela. Pourriez-vous nous donner plus de d√©tails ou contacter notre support afin que nous puissions r√©soudre le probl√®me ?\"\n",
    "\n",
    "\n",
    "\n",
    "üí° **Pour vous aider, vous pouvez suivre ces √©tapes :**\n",
    "\n",
    "1.  Cr√©ation d‚Äôune premi√®re cha√Æne : un prompt demande au LLM d‚Äôanalyser un commentaire client et de retourner uniquement le sentiment (positive, negative, neutral).\n",
    "2. Cr√©ation de trois fonctions (ou RunnableLambda) :\n",
    "- Pour r√©pondre positivement : remercier et encourager.\n",
    "- Pour r√©pondre √† un avis n√©gatif : exprimer des regrets, demander plus de d√©tails ou proposer de contacter le support.\n",
    "- Pour un avis neutre : offrir son aide et demander si le client souhaite en savoir plus.\n",
    "3. Utilisation de RunnableBranch pour appliquer le bon traitement selon le sentiment d√©tect√©.\n",
    "4. Regrouper le tout dans une cha√Æne compl√®te :\n",
    "- Entr√©e : un commentaire client (texte brut)\n",
    "- Sortie : une r√©ponse adapt√©e au ton d√©tect√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd4f0b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (0.3.25)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langchain) (0.3.63)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langchain) (0.3.44)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\user\\desktop\\langchain_runnable_chain\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\User\\Desktop\\LANGCHAIN_runnable_chain\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11c88080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment d√©tect√© : positive\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Le r√©sultat est **Positive**, merci pour votre retour positif ! Nous vous remercions chaleureusement üòä."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Votre code ici\n",
    "\n",
    "# === Prompt ===\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un expert en relation client√®le, en marketing et en analyse de sentiment.\"),\n",
    "    (\"human\", \"Analyse uniquement le sentiment de ce commentaire : {comment}. \"\n",
    "              \"rend un seul mot parmi ces options : positive, negative ou neutre. Retourne uniquement le mot, sans explications ou autres types de texte.\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "base_chain = prompt_template | model | parser\n",
    "\n",
    "sentiment = base_chain.invoke({\"comment\": \"c'est g√©nial. j'adore.\"})\n",
    "print(\"Sentiment d√©tect√© :\", sentiment)\n",
    "\n",
    "# === Runnables de traitement ===\n",
    "positive_run = RunnableLambda(lambda x: f\"Le r√©sultat est **{x}**, merci pour votre retour positif ! Nous vous remercions chaleureusement üòä.\")\n",
    "negative_run = RunnableLambda(lambda x: f\"Le r√©sultat est **{x}**, nous sommes d√©sol√©s pour cette exp√©rience. Pouvez-vous nous en dire plus ou contacter notre support ?\")\n",
    "neutre_run = RunnableLambda(lambda x: f\"Le r√©sultat est **{x}**, merci pour votre retour. Souhaitez-vous en savoir plus ou avez-vous besoin d'aide ?\")\n",
    "\n",
    "# === Branche conditionnelle avec fallback ===\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: x.lower() == \"positive\", positive_run),\n",
    "    (lambda x: x.lower() == \"negative\", negative_run),\n",
    "   neutre_run )\n",
    "# === Cha√Æne finale ===\n",
    "chain = base_chain | branch\n",
    "# === Test ===\n",
    "result = chain.invoke({\"comment\": \"c'est g√©nial. j'adore.\"})\n",
    "display(Markdown(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
